{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, confusion_matrix, precision_score, recall_score, auc\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(1)    # reproducible torch:2 np:3\n",
    "np.random.seed(1)\n",
    "\n",
    "from config import BIN_config_DBPE\n",
    "from models import BIN_Interaction_Flat\n",
    "from stream import BIN_Data_Encoder\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_generator, model):\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    loss_accumulate = 0.0\n",
    "    count = 0.0\n",
    "    for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n",
    "        # move inputs to device before forward\n",
    "        score = model(d.long().to(device), p.long().to(device), d_mask.long().to(device), p_mask.long().to(device))\n",
    "        \n",
    "        m = torch.nn.Sigmoid()\n",
    "        logits = torch.squeeze(m(score))\n",
    "        loss_fct = torch.nn.BCELoss()            \n",
    "        \n",
    "        label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
    "\n",
    "        loss = loss_fct(logits, label)\n",
    "        \n",
    "        loss_accumulate += loss\n",
    "        count += 1\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        y_label = y_label + label_ids.flatten().tolist()\n",
    "        y_pred = y_pred + logits.flatten().tolist()\n",
    "        \n",
    "    loss = loss_accumulate/count\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
    "\n",
    "    precision = tpr / (tpr + fpr)\n",
    "\n",
    "    f1 = 2 * precision * tpr / (tpr + precision + 0.00001)\n",
    "\n",
    "    thred_optim = thresholds[5:][np.argmax(f1[5:])]\n",
    "\n",
    "    print(\"optimal threshold: \" + str(thred_optim))\n",
    "\n",
    "    y_pred_s = [1 if i else 0 for i in (y_pred >= thred_optim)]\n",
    "\n",
    "    auc_k = auc(fpr, tpr)\n",
    "    print(\"AUROC:\" + str(auc_k))\n",
    "    print(\"AUPRC: \"+ str(average_precision_score(y_label, y_pred)))\n",
    "\n",
    "    cm1 = confusion_matrix(y_label, y_pred_s)\n",
    "    print('Confusion Matrix : \\n-', cm1)\n",
    "    print('Recall : ', recall_score(y_label, y_pred_s))\n",
    "    print('Precision : ', precision_score(y_label, y_pred_s))\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "\n",
    "    outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n",
    "    return roc_auc_score(y_label, y_pred), average_precision_score(y_label, y_pred), f1_score(y_label, outputs), y_pred, loss.item()\n",
    "\n",
    "\n",
    "def main(fold_n, lr):\n",
    "    config = BIN_config_DBPE()\n",
    "    \n",
    "    lr = lr\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    train_epoch = 100\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model = BIN_Interaction_Flat(**config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if use_cuda and torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model, dim = 0)\n",
    "            \n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    #opt = torch.optim.SGD(model.parameters(), lr = lr, momentum=0.9)\n",
    "    \n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 6, \n",
    "              'drop_last': True}\n",
    "\n",
    "    dataFolder = './dataset/BindingDB'\n",
    "    # dataFolder = \"./dataset/b_cancer/splitted\"\n",
    "    df_train = pd.read_csv(dataFolder + '/train.csv')\n",
    "    df_val = pd.read_csv(dataFolder + '/val.csv')\n",
    "    df_test = pd.read_csv(dataFolder + '/test.csv')\n",
    "    \n",
    "    training_set = BIN_Data_Encoder(df_train.index.values, df_train.Label.values, df_train)\n",
    "    training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = BIN_Data_Encoder(df_val.index.values, df_val.Label.values, df_val)\n",
    "    validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    testing_set = BIN_Data_Encoder(df_test.index.values, df_test.Label.values, df_test)\n",
    "    testing_generator = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    # early stopping\n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model)\n",
    "    \n",
    "    print('--- Go for Training ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    for epo in range(train_epoch):\n",
    "        model.train()\n",
    "        for i, (d, p, d_mask, p_mask, label) in enumerate(training_generator):\n",
    "            # move inputs to device before forward\n",
    "            score = model(d.long().to(device), p.long().to(device), d_mask.long().to(device), p_mask.long().to(device))\n",
    "\n",
    "            label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
    "            \n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss = loss_fct(n, label)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if (i % 100 == 0):\n",
    "                print('Training at Epoch ' + str(epo + 1) + ' iteration ' + str(i) + ' with loss ' + str(loss.cpu().detach().numpy()))\n",
    "            \n",
    "        # every epoch test\n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, auprc, f1, logits, loss = test(validation_generator, model)\n",
    "            if auc > max_auc:\n",
    "                model_max = copy.deepcopy(model)\n",
    "                max_auc = auc\n",
    "            \n",
    "            print('Validation at Epoch '+ str(epo + 1) + ' , AUROC: '+ str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1))\n",
    "    \n",
    "    print('--- Go for Testing ---')\n",
    "    try:\n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, auprc, f1, logits, loss = test(testing_generator, model_max)\n",
    "            print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Test loss: '+str(loss))\n",
    "    except:\n",
    "        print('testing failed')\n",
    "    return model_max, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 0.7057605\n",
      "Training at Epoch 1 iteration 0 with loss 0.7057605\n",
      "Training at Epoch 1 iteration 100 with loss 0.5906105\n",
      "Training at Epoch 1 iteration 100 with loss 0.5906105\n",
      "Training at Epoch 1 iteration 200 with loss 0.6139644\n",
      "Training at Epoch 1 iteration 200 with loss 0.6139644\n"
     ]
    }
   ],
   "source": [
    "# fold 1\n",
    "#biosnap interaction times 1e-6, flat, batch size 64, len 205, channel 3, epoch 50\n",
    "s = time()\n",
    "model_max, loss_history = main(1, 5e-6)\n",
    "e = time()\n",
    "print(e-s)\n",
    "lh = list(filter(lambda x: x < 1, loss_history))\n",
    "plt.plot(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moltrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
