{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, confusion_matrix, precision_score, recall_score, auc\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(1)    # reproducible torch:2 np:3\n",
    "np.random.seed(1)\n",
    "\n",
    "from config import BIN_config_DBPE\n",
    "from models import BIN_Interaction_Flat\n",
    "from stream import BIN_Data_Encoder\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_generator, model):\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    loss_accumulate = 0.0\n",
    "    count = 0.0\n",
    "    for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n",
    "        # move inputs to device before forward\n",
    "        score = model(d.long().to(device), p.long().to(device), d_mask.long().to(device), p_mask.long().to(device))\n",
    "        \n",
    "        m = torch.nn.Sigmoid()\n",
    "        logits = torch.squeeze(m(score))\n",
    "        loss_fct = torch.nn.BCELoss()            \n",
    "        \n",
    "        label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
    "\n",
    "        loss = loss_fct(logits, label)\n",
    "        \n",
    "        loss_accumulate += loss\n",
    "        count += 1\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        y_label = y_label + label_ids.flatten().tolist()\n",
    "        y_pred = y_pred + logits.flatten().tolist()\n",
    "        \n",
    "    loss = loss_accumulate/count\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
    "\n",
    "    try:\n",
    "        precision = tpr / (tpr + fpr)\n",
    "    except:\n",
    "        print(\"Precision error: tpr: \",tpr,\", fpr: \", fpr)\n",
    "        precision = tpr / (tpr + fpr + 0.00001) \n",
    "\n",
    "    f1 = 2 * precision * tpr / (tpr + precision + 0.00001)\n",
    "\n",
    "    thred_optim = thresholds[5:][np.argmax(f1[5:])]\n",
    "\n",
    "    print(\"optimal threshold: \" + str(thred_optim))\n",
    "\n",
    "    y_pred_s = [1 if i else 0 for i in (y_pred >= thred_optim)]\n",
    "\n",
    "    auc_k = auc(fpr, tpr)\n",
    "    print(\"AUROC:\" + str(auc_k))\n",
    "    print(\"AUPRC: \"+ str(average_precision_score(y_label, y_pred)))\n",
    "\n",
    "\n",
    "    pre = precision_score(y_label, y_pred_s)\n",
    "    rec = recall_score(y_label, y_pred_s)\n",
    "    cm1 = confusion_matrix(y_label, y_pred_s)\n",
    "    print('Confusion Matrix : \\n-', cm1)\n",
    "    print('Recall : ', recall_score(y_label, y_pred_s))\n",
    "    print('Precision : ', precision_score(y_label, y_pred_s))\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    print('Specificity : ', specificity1)\n",
    "\n",
    "    outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n",
    "    return accuracy1, pre, rec, roc_auc_score(y_label, y_pred), average_precision_score(y_label, y_pred), f1_score(y_label, outputs), y_pred, loss.item()\n",
    "\n",
    "\n",
    "def main(fold_n, lr):\n",
    "    config = BIN_config_DBPE()\n",
    "    \n",
    "    lr = lr\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    train_epoch = 100\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model = BIN_Interaction_Flat(**config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if use_cuda and torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model, dim = 0)\n",
    "            \n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    #opt = torch.optim.SGD(model.parameters(), lr = lr, momentum=0.9)\n",
    "    \n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 6, \n",
    "              'drop_last': True}\n",
    "\n",
    "    # dataFolder = './dataset/BindingDB'\n",
    "    dataFolder = \"./dataset/b_cancer/splitted\"\n",
    "    df_train = pd.read_csv(dataFolder + '/train.csv')\n",
    "    df_val = pd.read_csv(dataFolder + '/val.csv')\n",
    "    df_test = pd.read_csv(dataFolder + '/test.csv')\n",
    "    \n",
    "    training_set = BIN_Data_Encoder(df_train.index.values, df_train.Label.values, df_train)\n",
    "    training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = BIN_Data_Encoder(df_val.index.values, df_val.Label.values, df_val)\n",
    "    validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    testing_set = BIN_Data_Encoder(df_test.index.values, df_test.Label.values, df_test)\n",
    "    testing_generator = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    # early stopping\n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model)\n",
    "    \n",
    "    print('--- Go for Training ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    resultfile = open('results/b_cancer.txt', 'w')\n",
    "    resultfile.write('Epoch,AUROC,Accuracy,Precision,Recall,AUPRC,F1\\n')\n",
    "    \n",
    "    for epo in range(train_epoch):\n",
    "        model.train()\n",
    "        for i, (d, p, d_mask, p_mask, label) in enumerate(training_generator):\n",
    "            # move inputs to device before forward\n",
    "            score = model(d.long().to(device), p.long().to(device), d_mask.long().to(device), p_mask.long().to(device))\n",
    "\n",
    "            label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
    "            \n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss = loss_fct(n, label)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if (i % 100 == 0):\n",
    "                print('Training at Epoch ' + str(epo + 1) + ' iteration ' + str(i) + ' with loss ' + str(loss.cpu().detach().numpy()))\n",
    "            \n",
    "        # every epoch test\n",
    "        with torch.set_grad_enabled(False):\n",
    "            acc, pre, rec, auc, auprc, f1, logits, loss = test(validation_generator, model)\n",
    "            if auc > max_auc:\n",
    "                model_max = copy.deepcopy(model)\n",
    "                max_auc = auc\n",
    "            \n",
    "            print('Validation at Epoch '+ str(epo + 1) + ' :: , AUROC: '+ str(auc) + ' ',' Accuracy: ',acc,' , Precision: ',pre,' , Recall: ',rec,' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1))\n",
    "            resultfile.write(f'{epo + 1},{auc},{acc},{pre},{rec},{auprc},{f1}\\n')\n",
    "    resultfile.close()\n",
    "    \n",
    "    print('--- Go for Testing ---')\n",
    "    try:\n",
    "        with torch.set_grad_enabled(False):\n",
    "            acc, pre, rec, auc, auprc, f1, logits, loss = test(testing_generator, model_max)\n",
    "            print('Testing :: ','Accuracy: ',acc,' , Precision: ',pre,' , Recall: ',rec,' , AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Test loss: '+str(loss))\n",
    "    except:\n",
    "        print('testing failed')\n",
    "    return model_max, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 0.7316459\n",
      "Training at Epoch 1 iteration 0 with loss 0.7316459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_29200\\231881892.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = tpr / (tpr + fpr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: 0.42815282940864563\n",
      "AUROC:0.7445312500000001\n",
      "AUPRC: 0.7617311964885616\n",
      "Confusion Matrix : \n",
      "- [[47 33]\n",
      " [17 63]]\n",
      "Recall :  0.7875\n",
      "Precision :  0.65625\n",
      "Accuracy :  0.6875\n",
      "Sensitivity :  0.5875\n",
      "Specificity :  0.7875\n",
      "Validation at Epoch 1 :: , AUROC: 0.7445312500000001   Accuracy:  0.6875  , Precision:  0.65625  , Recall:  0.7875  , AUPRC: 0.7617311964885616 , F1: 0.6266666666666667\n",
      "Validation at Epoch 1 :: , AUROC: 0.7445312500000001   Accuracy:  0.6875  , Precision:  0.65625  , Recall:  0.7875  , AUPRC: 0.7617311964885616 , F1: 0.6266666666666667\n",
      "Training at Epoch 2 iteration 0 with loss 0.6334496\n",
      "Training at Epoch 2 iteration 0 with loss 0.6334496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_29200\\231881892.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  precision = tpr / (tpr + fpr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: 0.550786554813385\n",
      "AUROC:0.7590248476324425\n",
      "AUPRC: 0.7787297335383943\n",
      "Confusion Matrix : \n",
      "- [[61 18]\n",
      " [24 57]]\n",
      "Recall :  0.7037037037037037\n",
      "Precision :  0.76\n",
      "Accuracy :  0.7375\n",
      "Sensitivity :  0.7721518987341772\n",
      "Specificity :  0.7037037037037037\n",
      "Validation at Epoch 2 :: , AUROC: 0.7590248476324425   Accuracy:  0.7375  , Precision:  0.76  , Recall:  0.7037037037037037  , AUPRC: 0.7787297335383943 , F1: 0.7111111111111111\n",
      "Validation at Epoch 2 :: , AUROC: 0.7590248476324425   Accuracy:  0.7375  , Precision:  0.76  , Recall:  0.7037037037037037  , AUPRC: 0.7787297335383943 , F1: 0.7111111111111111\n",
      "Training at Epoch 3 iteration 0 with loss 0.7008484\n",
      "Training at Epoch 3 iteration 0 with loss 0.7008484\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# fold 1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#biosnap interaction times 1e-6, flat, batch size 64, len 205, channel 3, epoch 50\u001b[39;00m\n\u001b[0;32m      3\u001b[0m s \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m----> 4\u001b[0m model_max, loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m e \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(e\u001b[38;5;241m-\u001b[39ms)\n",
      "Cell \u001b[1;32mIn[11], line 132\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(fold_n, lr)\u001b[0m\n\u001b[0;32m    129\u001b[0m loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    131\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 132\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\envs\\moltrans\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\envs\\moltrans\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\miniconda3\\envs\\moltrans\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fold 1\n",
    "#biosnap interaction times 1e-6, flat, batch size 64, len 205, channel 3, epoch 50\n",
    "s = time()\n",
    "model_max, loss_history = main(1, 5e-6)\n",
    "e = time()\n",
    "print(e-s)\n",
    "lh = list(filter(lambda x: x < 1, loss_history))\n",
    "plt.plot(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moltrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
